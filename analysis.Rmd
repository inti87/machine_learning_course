---
title: "Human Activity Recognition - A Machine Learning Approach"
author: "Marko Intihar"
date: "12/11/2020"
output:
  html_document:
    keep_md: true
---

```{r setoptions, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, fig.width = 7, fig.height = 3,
                      warning = FALSE)
```

```{r firststep, message=FALSE, echo=FALSE}
rm(list = ls())
graphics.off()

# Load R packages
packages <- c("dplyr", "ggplot2", "caret", "janitor", "lubridate", "purrr", "kableExtra", "tidyr", "nnet", "naivebayes","rpart", "randomForest", "xgboost", "doParallel", "stringr") # list of packages to load
package.check <- lapply( # load or install & load list of packages
  packages,
  FUN = function(x) {
    if (!require(x, character.only = TRUE)) {
      install.packages(x, dependencies = TRUE)
      library(x, character.only = TRUE)
    }
  }
) 
rm(packages, package.check)
```


## Introduction

In this analysis we are focusing on data gathered from accelerometers, which were attached on participants' belt, forearm, arm, and dumbbell, and were colecting data regarding exercises. In the data collection process 6 participants were participating. The main goal of the analysis is to build a machine learning algorithm that will be able to predict the manner in which participants did the exercise using the data collected on accelometers.


## Data 

In this project data was provided by group of researchers who written publication titled **Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements** (reference here [url](http://groupware.les.inf.puc-rio.br/work.jsf?p1=10335)).


We have obtained two separate data files (stored in *.csv* format):

* **pmltraining.csv**
* **pmltesting.csv**


The case is we have to be fair and we will behave that we are not able to see **pmltesting.csv** file, which corresponds to new measurements we receive, and where we must predict the classes. Therefore given file will only be used for final model predictions.


* **train** data set (75% of measurements)
* **test** data set (25% of measurements)

Splitting will be done based on our outcome variable (**classe**) and doing a random shuffling of rows and then splitting into two parts (based on selected percentage). In the model training procedure we will use so called **k-fold cross-validation** technique om **train** data set, meaning our train data set will be randomly splitted into *k*-different folds and we will use *k-1* folds to train our model and check model performance on the fold left out We will repeat *k* iterations of training, in each iteration leaving out one new fold, therefore each fold will be once left out. By doing so we will get better model estimates and try to reduce the over-fitting of the model parameters. For data exploration we will also use only **train** data set. The **test** data set will be used for model benchmark, in order to select the top performing models.



### Data import

```{r dataimport}

# Check if data folder exists
if(!dir.exists("data")){
  dir.create("data")
}

# download csv files train/test
if(!file.exists("./data/pmltraining.csv")){
  download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                destfile = "./data/pmltraining.csv")
}
if(!file.exists("./data/pmltesting.csv")){
  download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                destfile = "./data/pmltesting.csv")
}


# import csv
pml_training <- read.csv(file = "./data/pmltraining.csv", header = T, 
                         sep = ",", quote = '"', row.names = 1) %>% 
  clean_names()
pml_testing <- read.csv(file = "./data/pmltesting.csv", header = T, 
                         sep = ",", quote = '"', row.names = 1) %>% 
  clean_names()
```



### Data split

Now let's do the splitting of **pml_training** data source using caret package.

```{r datasetsplit}
set.seed(11235) # seed for split
train_rows <- createDataPartition(y = pml_training$classe, p = .75, list = F) # do the splitting 

train <- pml_training[train_rows, ] # train data set
test  <- pml_training[-train_rows, ] # test data set
```




### Data wrangle


Now we will check if there are any columns that have a lot of missing values NAs (potential imputation or removing columns):

```{r NAs}
# calculate % of missing rows in each column 
NAs <- map(train, ~sum(is.na(.))) %>% 
  unlist() / nrow(train) 

# create a DF: var and % of missing values in that var
NAs <- data.frame(var = names(NAs),
                  missing = NAs) %>% 
  mutate(missing = round(missing * 100, digits = 1)) %>% 
  rename(`missing rows %` = missing) %>% 
  arrange(desc(`missing rows %`))
rownames(NAs) <- NULL

# extract names of variables to drop
drop.vars <- NAs %>% filter(`missing rows %` > 90) %>% pull(var) # list of columns to drop (more then 50 % NA)
```

There are `r length(drop.vars)` variables with almost all missing values. We will drop these variables, since there aren't any added value, if we put them into our modeling procedure. Lets drop columns from train and test data set:


```{r dropmissingvars}
train <- train %>% select(-drop.vars) 
test  <- test %>% select(-drop.vars) 
```


Let see which columns are character type (potential conversion to numeric or factor, or to drop columns). On a first sight we think we must have most of the columns numeric. Some strange values can prevent numeric variables to becoming numeric, lets find out which values are preventing this to happen. Isolate first character vectors:

```{r whichcolchar}
# check which columns are characters (potential transformation to numeric or to drop columns)
charvars <- colnames(train)[train %>% lapply(class) == "character"]
charvars
```

If we omit columns such as: "user_name", "cvtd_timestamp", "new_window", "classe", we think based on the column names we are dealing with numeric columns. Lets check check their summaries and try to find strange values:

```{r strangevalues}
strangevalues <- train[,charvars] %>% 
  select(-c("user_name", "cvtd_timestamp", "new_window", "classe")) 

# lets build a long format df: variable name and value 
# so we can do group by each variable and check unique values
# and display top ten occurence of values
strangevalues %>% 
  pivot_longer(cols = colnames(strangevalues)) %>% 
  group_by(value) %>% 
  count() %>% 
  ungroup() %>% 
  arrange(desc(n)) %>% 
  head(10)
```

As seen above a lot of missin values are written as blank string "", and also there are a lot of values "#DIV/0!" (probably some number division error). We will first force all this column to be converted to numeric, and then we will check missing values inside this columns, and finally remove some additional columns with a lot of missing values:

```{r forceconvchar}
vars.force.conv <- colnames(strangevalues) # list of variables to force conversion

# force conversion (train & test) - from char to num
train <- train %>% 
  mutate_at(.vars = vars.force.conv, .funs = as.numeric)
test <- test %>% 
  mutate_at(.vars = vars.force.conv, .funs = as.numeric)

```


Now we will check again the percentage of missing values for columns we converted from character to numeric in previous step:

```{r NAs2ndattempt}
# calculate % of missing rows in each column (observed columns)
NAs <- train[, vars.force.conv] %>% 
  map(., ~sum(is.na(.))) %>% 
  unlist() / nrow(train) 

# create a DF: var and % of missing values in that var
NAs <- data.frame(var = names(NAs),
                  missing = NAs) %>% 
  mutate(missing = round(missing * 100, digits = 1)) %>% 
  rename(`missing rows %` = missing) %>% 
  arrange(desc(`missing rows %`))
rownames(NAs) <- NULL

# extract names of variables to drop
drop.vars <- NAs %>% filter(`missing rows %` > 90) %>% pull(var) # list of columns to drop (more then 50 % NA)

# show percentage of missing rows
NAs %>% 
  kbl() %>% 
  kable_paper() %>%
  scroll_box(width = "400px", height = "500px")
  

```

Table above shows % of misisng rows for forcefully converted columns. As you can see, all columns have almost all missing values (due to blank strings and other characters). So we will also remove all this columns, since they do not bring any added value to the table:

```{r dropmissingvars2ndattempt}
train <- train %>% select(-drop.vars) 
test  <- test %>% select(-drop.vars) 
```


Finally lets apply some essential variable data types transformations (on test and train data sets). We have to transform outcome variable to factor, and also there are some date time specific columns:

```{r wrangle}
train <- train %>%
  mutate(classe = as.factor(classe), # convert outcome to factor variable
         cvtd_timestamp = dmy_hm(cvtd_timestamp)) # convert to date time object
test <- test %>%
  mutate(classe = as.factor(classe), # convert outcome to factor variable
         cvtd_timestamp = dmy_hm(cvtd_timestamp)) # convert to date time object
```




## Exploratory Data Analysis (EDA)

First lets check how many users are included in the data:
```{r different users}
train %>% 
  count(user_name)
```

Now lets check summary of date & date-time related variables:

```{r summarydt}
# min/max raw timestamp part 1
train %>% 
  group_by(user_name) %>% 
  summarise(min_raw_ts1 = min(raw_timestamp_part_1),
            max_raw_ts1 = max(raw_timestamp_part_1))

# min/max raw timestamp part 2
train %>% 
  group_by(user_name) %>% 
  summarise(min_raw_ts2 = min(raw_timestamp_part_2),
            max_raw_ts2 = max(raw_timestamp_part_2))

```
We think date and date-time related columns are irrelevant for model we are building (that will predict **classe** outcome). Therefore we will drop columns:

* **raw_timestamp_part_1**
* **raw_timestamp_part_2**
* **cvtd_timestamp**

Also we will drop column called **new_window**, but at this point we will keep column **num_window**.

```{r dropmissingvars3rdattempt}
drop.vars <- c("raw_timestamp_part_1", "raw_timestamp_part_2",
               "cvtd_timestamp", "new_window")
train <- train %>% select(-drop.vars) 
test  <- test %>% select(-drop.vars) 
```

Now lets check how numerical variables are correlated between themselves. We will highlight predictor pairs with high correlation. High correlation might indicate some multicollinearity among predictors, but we hope that this phenomena won't cause any troubles due to selected prediction algorithms.

```{r correlation}
# we check correlation for only numeric variables
relevant.vars <- colnames(train)[train %>% lapply(class) %in% c("numeric", "integer")]

cor.mat <- cor(train[, relevant.vars]) # correlation matrix
diag(cor.mat) <- 0 # set diagonal elements to zero
cor.mat[upper.tri(cor.mat)] <- 0 # set upper triangular elements to zero

# Top correlation
inds <- which(abs(cor.mat) > 0.8, arr.ind = TRUE) # index

# cerate table of pairs
cor.top <- data.frame(Var1 = rownames(cor.mat)[inds[, 1]], 
                      Var2 = colnames(cor.mat)[inds[, 2]], 
                      Cor = cor.mat[inds]) %>% 
  distinct() %>% 
  arrange(desc(abs(Cor)))
```

Now lets list predictors and their correlations:
```{r topcor}
cor.top
```

The table above shows pairs of predictors that are highly correlated (in positive or negative direction). We hope this will not cerate any problems for selected prediction algorithms. 

Now lets check how balanced is occurrence of each value in outcome variable (**classe**). Since we have a multi-class classification problem, we must select the adequate performance measure (it can be slightly more complicated than the case with a binary class outcome!):

```{r outcomevalues}
train %>% 
  group_by(classe) %>% 
  count()
```


We can see that classes "B", "C", "D", "E" are more than less balanced, but class "A" has slightly more observations compared to other classes. 



## Modeling

In the modeling stage we would like to fit parameters of selected model using train data set. In order to predict outcome variable using test data set (and later validation or final test data set). Selected model types are (keep in mind we are dealing with multi- class classification problem!):

* Multinomial logistic regression classification algorithm (R package - **nnet**)
* Naive Bayes classification algorithm (R package **naivebayes**)
* k-nearest neighbors (KNN) classification algorithm (inside **caret** package)
* Recursive Partitioning And Regression Trees classification algorithm (R package - **rpart**)
* Random Forests classification algorithm (R package *rf*)
* Xgboost ~ eXtreme Gradient Boosting classification algorithm (R package **xgboost**)


First lets set control environment for each model (we are using 10-fold cross-validation in the model fitting stage) and also we would:

```{r controlsetting}
## Set control parameters for caret package
#  - cross validations settings (number of folds)
#  - type of output of models - prediction + probabilities 

nr.CV_folds <- 10 # number of folds - cross validation

# base setting
control_setting_ <- trainControl(method ="cv",  
                                number = nr.CV_folds,
                                classProbs = T)
```


### Model training

Some model fit procedure uses parallel computing, if you will use the RMarkdown code please set number of cores according to your PV (where you will evaluate the code !).

Now lets execute model training (fit model parameters) using train dataset:

```{r modelfit, cache=TRUE}


# Model fit

## k-nearest neighbors (KNN)
set.seed(11235)
knn_time_start <- Sys.time() # time start
knn_fit <- train(classe ~ ., 
                 data = train, 
                 method = "knn", 
                 trControl = control_setting_)
knn_time_end <- Sys.time() # time end


## Recursive Partitioning And Regression Trees 
set.seed(11235)
rprt_time_start <- Sys.time() # time start
rprt_fit <- train(classe ~ ., 
                  data = train, 
                  method = "rpart", 
                  trControl = control_setting_)
rprt_time_end <- Sys.time() # time end


#----------------------------------------#
## Parallel core processing ..... begin
#----------------------------------------#

## All subsequent models are then run in parallel
nc <- 16 # number of cores
#cl <- makeCluster(nc) # set cores number (to be used)
cl <- makePSOCKcluster(nc) # set cores number 
registerDoParallel(cl) 


## Multinomial logistic regression
set.seed(11235)
mlr_time_start <- Sys.time() # time start
mlr_fit <- train(classe ~ ., 
                 data = train, 
                 method = "nnet", 
                 trControl = control_setting_,
                 allowParallel = T)
mlr_time_end <- Sys.time() # time end


## Naive Bayes
set.seed(11235)
nb_time_start <- Sys.time() # time start
nb_fit <- train(classe ~ ., 
                data = train, 
                method = "nb", 
                trControl = control_setting_,
                allowParallel = T)
nb_time_end <- Sys.time() # time end


## Random Forests
set.seed(11235)
rf_time_start <- Sys.time() # time start
rf_fit <- train(classe ~ ., 
                data = train, 
                method = "rf", 
                trControl = control_setting_,
                allowParallel = T)
rf_time_end <- Sys.time() # time end



## Xgboost ~ eXtreme Gradient Boosting
set.seed(11235)
xgb_time_start <- Sys.time() # time start
xgb_fit <- train(classe ~ ., 
                 data = train, 
                 method = "xgbTree",
                 trControl = control_setting_,
                 allowParallel = T)
xgb_time_end <- Sys.time() # time end


## When you are done:
on.exit(stopCluster(cl))
env <- foreach:::.foreachGlobals
rm(list=ls(name=env), pos=env)
  

#----------------------------------------#
## Parallel core processing ..... end
#----------------------------------------#

```

Time spent to fit each model are shown below:

```{r modelfittime}
# Gather model estimation running time
model.fit.time <- tibble(model = c("k-nearest neighbors", "Recursive Partitioning And Regression Trees", "Multinomial logistic regression", "Naive Bayes", "Random Forests", "Xgboost"),
                            `multi-core estimation` = c(F, F, T, T, T, T),
                            `time in sec` = c(round(difftime(knn_time_end, knn_time_start, units = "secs"),0),
round(difftime(rprt_time_end, rprt_time_start, units = "secs"),0),
round(difftime(mlr_time_end, mlr_time_start, units = "secs"),0),
round(difftime(nb_time_end, nb_time_start, units = "secs"),0),
round(difftime(rf_time_end, rf_time_start, units = "secs"),0),
round(difftime(xgb_time_end, xgb_time_start, units = "secs"),0)))

# show time
model.fit.time %>% 
  kbl() %>% 
  kable_paper() %>%
  scroll_box(width = "100%", height = "100%")

```



### Model benchmark

First lets use our fitted model and predict the outcome variable (**classe**) on the test data set:

```{r modelprediction, cache=TRUE}

# model predictions
pred_knn  <- predict(knn_fit, newdata = test)
pred_rprt <- predict(rprt_fit, newdata = test)
pred_mlr  <- predict(mlr_fit, newdata = test)
pred_nb   <- predict(nb_fit, newdata = test)
pred_rf   <- predict(rf_fit, newdata = test)
pred_xgb  <- predict(xgb_fit, newdata = test)

```


Now let's calculate performance measures for each model, using predicted values of the outcome variable and actual values (on the test data set):

```{r modelperf}

# perfomance measures 
perf_knn  <- confusionMatrix(pred_knn, test$classe)
perf_rprt <- confusionMatrix(pred_rprt, test$classe)
perf_mlr  <- confusionMatrix(pred_mlr, test$classe)
perf_nb   <- confusionMatrix(pred_nb, test$classe)
perf_rf   <- confusionMatrix(pred_rf, test$classe)
perf_xgb  <- confusionMatrix(pred_xgb, test$classe)

```

For model benchmark we will be using given model performance measures:

* **Balanced Accuracy**
* Overall **Sensitivity**
* Overall **Specificity**

All measures are calculated for each model and each observed class of the outcome variable ("A", "B", "C", "D", "E"). **Balanced Accuracy** will tell us how accurate is model to predict the right class of the outcome variable. Overall **Sensitivity** will tell us probability that model predicted class = $i$ when the actual class is $i$. And overall **Specificity** will tell us probability that model predicted class not equal to $i$ when the actual class is not equal to $i$. We are gonna build a data frame of measures, therefore we will be able to visualize modle benchmark:

```{r model benchmarkprep}

# Create a DFs (individual)

# df.knn <- perf_knn$byClass[, c("Balanced Accuracy", "Sensitivity", "Specificity")] %>% 
#   as.data.frame() %>% 
#   mutate(class = classes,
#          model = "k-nearest neighbors") %>% 
#   pivot_longer(cols = c("Balanced Accuracy", "Sensitivity", "Specificity"),
#                names_to = "metric", values_to = "value") %>% 
#   select(model, class, everything()) %>%
#   mutate(class = as.factor(class))
# 
# df.rprt <- perf_rprt$byClass[, c("Balanced Accuracy", "Sensitivity", "Specificity")] %>% 
#   as.data.frame() %>% 
#   mutate(class = classes,
#          model = "Recur. Part. Reg. Trees") %>% 
#   pivot_longer(cols = c("Balanced Accuracy", "Sensitivity", "Specificity"),
#                names_to = "metric", values_to = "value") %>% 
#   select(model, class, everything()) %>%
#   mutate(class = as.factor(class))
# 
# df.mlr <- perf_mlr$byClass[, c("Balanced Accuracy", "Sensitivity", "Specificity")] %>% 
#   as.data.frame() %>% 
#   mutate(class = classes,
#          model = "Multi. log. reg.") %>% 
#   pivot_longer(cols = c("Balanced Accuracy", "Sensitivity", "Specificity"),
#                names_to = "metric", values_to = "value") %>% 
#   select(model, class, everything()) %>%
#   mutate(class = as.factor(class))
# 
# df.nb <- perf_nb$byClass[, c("Balanced Accuracy", "Sensitivity", "Specificity")] %>% 
#   as.data.frame() %>% 
#   mutate(class = classes,
#          model = "Naive Bayes") %>% 
#   pivot_longer(cols = c("Balanced Accuracy", "Sensitivity", "Specificity"),
#                names_to = "metric", values_to = "value") %>% 
#   select(model, class, everything()) %>%
#   mutate(class = as.factor(class))
# 
# df.rf <- perf_rf$byClass[, c("Balanced Accuracy", "Sensitivity", "Specificity")] %>% 
#   as.data.frame() %>% 
#   mutate(class = classes,
#          model = "Random Forests") %>% 
#   pivot_longer(cols = c("Balanced Accuracy", "Sensitivity", "Specificity"),
#                names_to = "metric", values_to = "value") %>% 
#   select(model, class, everything()) %>%
#   mutate(class = as.factor(class))
# 
# df.xgb <- perf_xgb$byClass[, c("Balanced Accuracy", "Sensitivity", "Specificity")] %>% 
#   as.data.frame() %>% 
#   mutate(class = classes,
#          model = "Xgboost") %>% 
#   pivot_longer(cols = c("Balanced Accuracy", "Sensitivity", "Specificity"),
#                names_to = "metric", values_to = "value") %>% 
#   select(model, class, everything()) %>%
#   mutate(class = as.factor(class))
# 
# 
# # Merged dfs
# df.perf <- bind_rows(df.knn, df.rprt, df.mlr, df.nb, df.rf, df.xgb) %>% 
#   mutate(model = as.factor(model),
#          class = as.factor(class),
#          metric = factor(metric, levels = c("Balanced Accuracy", "Sensitivity", "Specificity")))

```

Now lets visualize the results (first accuracy):

```{r perfacc}
df.perf %>% 
  filter(metric == "Balanced Accuracy") %>% 
  ggplot(aes(x = class, y = value, color = model, group = model)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  scale_color_viridis_d() +
  ggtitle("Balanced Accuracy (for each class) - test data set")
  
```
 
As we can see from the figure above, XGboost algorithm and Random Forests algorithm are quite dominant regarding accuracy (both are very close to 100% accuracy for selected classes). Now lets see Sensitivity and Specificity:

```{r perfsenspec}
df.perf %>% 
  filter(metric != "Balanced Accuracy") %>% 
  ggplot(aes(x = class, y = value, color = model, group = model)) +
  geom_line(size = 1) +
  geom_point(size = 3) +
  facet_grid(metric ~ .) +
  scale_color_viridis_d() +
  ggtitle("Sensitivity & Specificity (for each class) - test data set")
  
```
The second figure also shows that Random Forests algorithm and Xgboost algorithm are best regarding Sensitivity and Specificity. Now lets draw a final figure showing all metrics on one figure (average metric values over all classes):

```{r perfallshow}
df.perf %>% 
  group_by(model, metric) %>% 
  summarise(`Average value` = mean(value)) %>% 
  pivot_wider(id_cols = "model", 
              names_from = "metric", 
              values_from = "Average value") %>% 
  ggplot(aes(x = Sensitivity, y = Specificity, 
             size = `Balanced Accuracy`, fill = model)) +
  geom_point(shape = 21, color = "black") +
  scale_fill_viridis_d() +
  ggtitle("Selected metrics - average values (over classes)")

```
We will check top 2 performers using table, to be certain which algorithm to pick for final prediction (on test-validation set):

```{r topperformersmetrics}
df.perf %>% 
  filter(model %in% c("Random Forests", "Xgboost")) %>% 
  group_by(model, metric) %>% 
  summarise(`Average value` = mean(value)) %>% 
  pivot_wider(id_cols = "model", names_from = "metric", values_from = "Average value")
```

The model predictions are very good and very close, but by a very small difference we can see that model **Xgboost** algorithm outperformed **Random Forests** algorithm. Therefore our final selected model is **Xgboost**.


## Results (final prediction)

